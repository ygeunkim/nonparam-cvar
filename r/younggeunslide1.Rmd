---
title: "Conditional Expected Shortfall"
subtitle: "Nonparametric Estimation"
author: |
  | [Young-geun Kim](mailto:dudrms33@g.skku.edu)
  | [ygeunkim.github.io](https://ygeunkim.github.io)
  | 2019711358, [Department of Statistics](https://stat.skku.edu/stat/index.jsp)
date: "`r format(Sys.time(), '%d %b, %Y')`"
bibliography: "../docs/nonparam.bib"
output: 
  beamer_presentation:
    toc: yes
    slide_level: 2
    theme: "Malmoe"
    colortheme: "dolphin"
    fonttheme: "structurebold"
knit:
  (function(inputFile, encoding) {
    rmarkdown::render(input = inputFile, encoding = encoding, output_dir = "../static/slides")
  })
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{wrapfig}
  - \usepackage{float}
  - \usepackage{colortbl}
  - \usepackage{pdflscape}
  - \usepackage{tabu}
  - \usepackage{threeparttable}
  - \usepackage{threeparttablex}
  - \usepackage[normalem]{ulem}
  - \usepackage[normalem]{ulem}
  - \usepackage[utf8]{inputenc}
  - \usepackage{makecell}
  - \usepackage{xcolor}
  - \usepackage{hyperref}
  - \usepackage{enumitem}
  - \usepackage[boxruled, linesnumbered]{algorithm2e}
  - \IncMargin{1.5em}
  - \newcommand{\iid}{\stackrel{iid}{\sim}}
  - \newcommand{\indep}{\stackrel{indep}{\sim}}
  - \newcommand{\hsim}{\stackrel{H_0}{\sim}}
  - \newcommand{\ind}{\perp\!\!\!\perp}
  - \newcommand{\R}{\mathbb{R}}
  - \newcommand{\B}{\boldsymbol\beta}
  - \newcommand{\hb}{\boldsymbol{\hat\beta}}
  - \newcommand{\E}{\boldsymbol\epsilon}
  - \newcommand{\defn}{\mathpunct{:}=}
  - \DeclareMathOperator*{\argmin}{argmin}
  - \DeclareMathOperator*{\argmax}{argmax}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  out.width = "70%",
  fig.align = "center",
  fig.width = 6,
  fig.asp = .618,
  fig.pos = "H"
  )
knitr::knit_hooks$set(
  document = function(x) {
    sub("\\usepackage[]{color}", "\\usepackage{xcolor}", x, fixed = TRUE)
  }
)
options(digits = 3)
options(kableExtra.latex.load_packages = FALSE)
pander::panderOptions("digits", 3)
is_latex <- knitr::opts_knit$get("rmarkdown.pandoc.to") == "latex"
is_beamer <- knitr::opts_knit$get("rmarkdown.pandoc.to") == "beamer"
```

```{r, message=FALSE, echo=FALSE}
# tidyverse family---------------------
library(tidyverse)
# large data
library(data.table)
# custom packages----------------------
library(rmdtool) # devtools::install_github("ygeunkim/rmdtool")
library(goodgraphic) # devtools::install_github("ygeunkim/goodgraphic")
# set seed for report -----------------
set.seed(1)
```

# Expected Shortfall

## Value at Risk

```{r, echo=FALSE}
tibble(
  x = seq(-3, 3, by = .01),
  y = dnorm(x),
  VaR = x <= qnorm(.05)
) %>% 
  ggplot(aes(x = x)) +
  geom_path(aes(y = y)) +
  geom_ribbon(aes(ymin = 0, ymax = y, fill = VaR), show.legend = FALSE) +
  scale_fill_manual(values = c("TRUE" = gg_hcl(1), "FALSE" = NA))
```

@Tsay:2010aa says that

- Measure of loss under *normal* market conditions
- Minimal loss under *extraordinary* market circumstances

## Value at Risk

- $p$: **Right** tail probability
- $l$: Time horizon
- $L(l)$: loss function of the asset
- $F$: CDF of the loss

$$p = P \left[ L(l) \ge VaR \right]$$

## Subadditivity

### Coherent risk measure

- Homogeneity
- Monotonicity
- Translation invariance (risk-free condition)
- Subadditivity

### VaR

does not satisfy subadditivity

- When two portfolios are merged, the risk measure should not be greater than the sum of each.
- VaR *underestimates* the actual loss.

## Conditional VaR

- Stationary log-return $\{ Y_t \mathpunct{:} t = 1, \ldots n \}$
- Exogenous variable $\{ X_t \mathpunct{:} t = 1, \ldots n \}$
- Conditional VaR (CVaR) or Expected Shortfall (ES)

$$\nu_p(x) = S^{-1}(p \mid x)$$

where

- $S (y \mid x) \defn 1 - F(y \mid x)$
- $F$: conditional CDF of $Y_t$ given $X_t = x$.

## Conditional Expected Shortfall

- We are interested in *Expected Shortfall given exogenous variable values*
- Conditional Expected Shortfall (CES)

$$\mu_p(x) = E \left[ Y_t \mid Y_t \ge \nu_p(x), X_t = x \right]$$

## Formulating CES

Let $B \equiv \left\{ \omega \mathpunct{:} Y_t \ge \nu_p(x) \right\} \in \mathcal{B}$. Then

\begin{equation*}
  \begin{split}
    \mu_p(x) & = E \left[ Y_t \mid Y_t \ge \nu_p(x), X_t = x \right] \\
    & = \frac{1}{P(B)} \int_{B} Y_t dP \\
    & = \frac{1}{P\left( Y_t \ge \nu_p(x) \mid X_t = x \right)} \int_{\nu_p(x)}^\infty y f(y \mid x) dy \\
    & = \frac{1}{p} \int_{\nu_p(x)}^\infty y f(y \mid x) dy
  \end{split}
\end{equation*}

# Nonparametric Estimation

## Workflow of Estimation

### Plugging-in Method

$$\hat\mu_p(x) = \frac{1}{p} \int_{\hat\nu_p(x)}^\infty y \hat{f}(y \mid x) dy$$

### What to estimate

- Conditional PDF: $\hat{f}(y \mid x)$
- CVaR: $\hat\nu_p(x) = \hat{S}^{-1}(p \mid x)$ by inverting the conditional CDF

## Conditional Disribution

### Taylor expansion

Consider any symmetric kernel $K_h(\cdot)$. Then

\begin{equation*}
  \begin{split}
    E [ K_h(y - Y_t) & \mid X_t = x ] = K_h \ast f_{y \mid x} (y) \\
    & = f(y \mid x) + \frac{h^2}{2} \mu_2(K) f^{(2)}(y \mid x) + o(h^2)
  \end{split}
\end{equation*}

where $\mu_j(K) = \int_{\R}u^j K(u) du$.

### Smoothing

$$f(y \mid x) \approx E \left[ K_h(y - Y_t) \mid X_t = x \right]$$

## Methods

- Local Linear
- Weighted Nadaraya Watson
- WDKLL [@cai:2008aa]

## Double Kernel Local Linear

Denote $Y_t^{\ast}(y) \equiv K_h(y - Y_t)$.

$$\hat{f}(y \mid x) = \argmin_{\alpha(x), \beta(x)} \sum_{t = 1}^n W_\lambda(x - X_t) \left[ Y_t^{\ast}(y) - \alpha(x) - \beta(x) (X_t - x) \right]^2$$

Since this is involved in the two kernel ($K_h(\cdot)$, $W_\lambda(\cdot)$), @cai:2008aa names this as *double kernel*.

## Local Linear Solution

Note that the local linear estimate is equivalent to WLS.

- $\mathbf{Y}_y^{\ast} = \left( Y_1(y), \ldots, Y_n(y) \right)^T \in \R^n$
- $\mathbf{b}_x(x_t) \defn (1, x_t - x)^T \in \R^2$ and $\mathbf{b}_x(x) = \mathbf{e}_1 \defn (1, 0)^T$
- $X_x \defn \left( \mathbf{b}_x(x_i)^T \right) \in \R^{n \times 2}$
- $W_x \defn diag(W_\lambda(x - X_j)) \in \R^{n \times n}$

Then $\hat{f}_{ll} = \hat\alpha$:

\begin{equation*}
  \begin{split}
    \hat{f}_{ll}(y \mid x) & = \mathbf{e}_1^T (X_x^T W_x X_x)^{-1} X_x^T W_x \mathbf{Y}_y^{\ast} \\
    & = \mathbf{l}(x)^T \mathbf{Y}_y^{\ast} \\
    & \equiv \sum_{t = 1}^n l_t(x) Y_t^{\ast}(y)
  \end{split}
\end{equation*}

## Linear Smoother

$$\mathbf{l}(x)^T = \mathbf{e}_1^T (X_x^T W_x X_x)^{-1} X_x^T W_x$$

By annoying arithmetic,

$$l_t (x) = \frac{S_2(x) - (X_t - x) S_1(x)}{S_0(x) S_2(x) - \left[ S_1(x) \right]^2} W_\lambda(x - X_t)$$

where $S_j(x) \defn \sum\limits_{t = 1}^n W_\lambda(x - X_t) (X_t - x)^j$.

---

### Matrix computations

Let $w_t \equiv W_\lambda(x - X_t)$

$$
(X_x^T W_x X_x) = \left[\begin{array}{cc}
  \sum_t w_t & \sum_t w_t (x_t - x) \\
  \sum_t w_t (x_t - x) & \sum_t w_t (x_t - x)^2
\end{array}\right] \equiv \left[\begin{array}{cc}
  S_0 & S_1 \\
  S_1 & S_2
\end{array}\right]
$$

$$
X_x^T W_x = \left[\begin{array}{ccc}
  w_1 & \cdots & w_n \\
  w_1 (x_1 - x) & \cdots & w_n (x_n - x)
\end{array}\right]
$$

Thus,

$$
\mathbf{l}(x)^T = \frac{1}{S_0S_2 - S_1^2} \left[\begin{array}{ccc}
  S_2 w_1 - S_1 w_1(x_1 - x) & \cdots & S_2 w_n - S_1 w_n(x_n - x)
\end{array}\right]
$$

## Discrete Moments Conditions

$$
S_j(x) \defn \sum\limits_{t = 1}^n W_\lambda(x - X_t) (X_t - x)^j = \delta_{0,j} = \begin{cases}
  1 & j = 0 \\
  0 & \text{o/w}
\end{cases}
$$

will be used when showing the asymptotic properties

## CVaR

Invert $\hat{F}_{ll}(y \mid x)$

### Conditional CDF

\begin{equation*}
  \begin{split}
    \hat{F}_{ll}(y \mid x) & = \int_\infty^y \hat{f}_{ll}(y \mid x) dy \\
    & = \sum_{t = 1}^n l_t(x) G_h(y - Y_t)
  \end{split}
\end{equation*}

where $G(\cdot)$ is the cdf of $K(\cdot)$.

### Problem

- It must be $\hat{F}_{ll} \in [0, 1]$ and monotone increasing
- However, LL does not guarantee these properties.

## Weighted Nadaraya Watson

To get the right shape of CDF

$$\hat{F}_{NW}(y \mid x)  = \sum_{t = 1}^n H_t(x, \lambda) I(Y_t \le y)$$

where

$$H_t(x, \lambda) = \frac{p_t(x) W_\lambda(x - X_t)}{\sum\limits_{i = 1}^n p_i(x) W_\lambda(x - X_i)}$$

- $p_t(x)$ is *weighted* for each NW weight.
- @cai2001weighted finds the best weights $\{ p_t \}_1^n$ by maximizing the *empirical likelihood*.

## Choosing weights

### Constraints

- $p_t(x) \ge 0$
- $\sum_t p_t(x) = 1$
- Discrete moments conditions $\sum\limits_{t = 1}^n H_t(x, \lambda) (X_t - x)^j = \delta_{0,j}, \; 0 \le j \le 1$

### Empirical likelihood

Maximize $\sum_t \ln p_t(x)$. Lagrangian multiplier gives that

$$p_t(x) = \frac{1}{n \left[ 1 + \gamma(X_t - x) W_\lambda(x - X_i) \right]} \ge 0$$

and $\gamma$ uniquely maximizing the log of the empirical likelihood

$$L_n(\gamma) = - \sum_{t = 1}^n \ln \left[ 1 + \gamma(X_t - x) W_\lambda(x - X_i) \right]$$

## Weighted Double Kernel Local Linear

- In a local linear scheme,
- replace linear smoother with WNW weight

$$\hat{f}_{cai}(y \mid x) = \sum_{t = 1}^n H_t(x, \lambda) Y_t^{\ast}(y)$$

and hence,

\begin{equation*}
  \begin{split}
    \hat{F}_{cai}(y \mid x) & = \int_\infty^y \hat{f}_{cai}(y \mid x) dy \\
    & = \sum_{t = 1}^n H_t(x, \lambda) G_h(y - Y_t)
  \end{split}
\end{equation*}

## Inverting and Plugging-in

### CVaR

$$\hat\nu_p^{(cai)}(x) = \hat{S}_{cai}^{-1}(p \mid x)$$

where $\hat{S}_{cai}(y \mid x) = 1 - \hat{F}_{cai}(y \mid x)$

### CES

$$\hat\mu_p(x) = \frac{1}{p} \sum_{t = 1}^n H_t(x, \lambda) \left[ Y_t \bar{G}_h (\hat\nu_p(x) - Y_t) + h G_{1, h}(\hat\nu_p(x) - Y_t) \right]$$

where $\bar{G}(u) = 1 - G(u)$ and $G_1(u) = \int_u^\infty  v K(v)  dv$.


# Statistical Properties

## Asymptotic Normality

### Investigate

- $\hat{f}_{cai} (y \mid x)$
- $\hat{S}_{cai} (y \mid x) = 1 - \hat{F}_{cai}(y \mid x)$
- $\hat\nu_p(x)$
- $\hat\mu_p(x)$

### at both

| Interior | Boundary |  
|:--------:|:--------:|  
| $x$ | $x = c\lambda$ |  


## Notations

- $\alpha(K) = \int_{-\infty}^{\infty} u K(u) \bar{G}(u) du$
- $\mu(W) = \int_{-\infty}^{\infty} u^j W(u) du$
- $l_j(u \mid v) = E \left[ Y_t^j I(Y_t \ge u) \mid X_t = v \right]$
- $l_j^{a,b}(u \mid v) = \frac{\partial^{ab}}{\partial u^a \partial v^b} l_j(u \mid v)$

## Interior

$$\sqrt{n \lambda} \left[ \hat\mu_p(x) - \mu(x) - B_{\mu}(x) \right] \stackrel{\mathcal{D}}{\longrightarrow} N \left( 0, \sigma_{\mu}^2(x) \right)$$

## Boundary

W.L.O.G. the left boundary point $x = c\lambda$ s.t.

- $spt K = \left[ -1, 1 \right]$
- $c \in (0, 1)$

$$\sqrt{n \lambda} \left[ \hat\mu_p(c\lambda) - \mu(c\lambda) - B_{\mu, c} \right] \stackrel{\mathcal{D}}{\longrightarrow} N \left( 0, \sigma_{\mu, c}^2 \right)$$

# Simulation for Asymptotic Normality

## Codes

```{r, message=FALSE}
# devtools::install_github("ygeunkim/ceshat")
library(ceshat)
# devtools::install_github("ygeunkim/youngtool")
library(youngtool)
# GARCH modeling
library(fGarch)
```

For details, see my Github package repositories^[[github.com/ygeunkim/ceshat](https://github.com/ygeunkim/ceshat) and [github.com/ygeunkim/youngtool](https://github.com/ygeunkim/youngtool)]

## Models

### AR(1)-GARCH(1, 0)

$$
\begin{cases}
  X_t = Y_{t - 1} \\
  Y_t = 0.01 + 0.62 X_t + \sigma_t \epsilon_t \\
  \sigma_t^2 = -0.15 + 0.65 \sigma_{t - 1}^2 \\
  \epsilon_t \sim N(0, 1)
\end{cases}
$$

## Random number generation

Monte Carlo Samples:

- For fixed $x_t$
- Generate GARCH(1, 0): $(\sigma_t, \epsilon_t)$
- $X_t = Y_{t - 1}$
- AR(1): $Y_t = 0.01 + 0.62 Y_{t - 1} + \sigma_t \epsilon_t$

---

```{r}
garch_sim <- function(n, cond, ar_mu = .01, ar = .62) {
  garch_spec <- 
    garchSpec(
      cond.dist = "norm",
      model = list(
        omega = .15, alpha = 0, beta = .65
      )
    )
  tibble(garch = garchSim(garch_spec, n = n) %>% as.numeric()) %>% 
    mutate(
      x = cond,
      y = ar_mu + ar * x + garch
    ) %>% 
    select(y) %>% # to use youngtool (experimental stage)
    pull()
}
```


## Monte Carlo Simulation


```{r}
cond_sim <- function(n, m, xcond) {
  mc_data(garch_sim, N = n, M = m, cond = xcond) %>% 
    .[,
      xcond := xcond] %>% 
    .[]
}
```


## Empirical Distribution

```{r}
x <- runif(1)
mc <- cond_sim(200, 500, x)
```

## Interior

At $x = `r x`$,

```{r, cache=TRUE}
CES <- 
  mc[,
     .(mu =
         wdkll_ces(x ~ xcond, .SD) %>% 
         predict(newx = unique(xcond))),
     by = mc]
```

---

```{r, echo=FALSE}
CES %>% 
  ggplot(aes(x = mu)) +
  geom_histogram(alpha = .7, binwidth = .005)
```

- Empirical distribution of $\hat\mu_p(`r x`)$
- Shape of Normal distribution

## Boundary

```{r}
bound_c <- runif(1) * 200^(-4/5)
mc2 <- cond_sim(200, 500, bound_c)
```

At $x = `r bound_c`$,

```{r, cache=TRUE}
CES2 <- 
  mc2[,
      .(mu =
          wdkll_ces(x ~ xcond, .SD) %>% 
          predict(newx = unique(xcond))),
      by = mc]
```

---

```{r, echo=FALSE}
CES2 %>% 
  ggplot(aes(x = mu)) +
  geom_histogram(alpha = .7, binwidth = .005)
```

- Empricial distribution of $\hat\mu_p(x)$ at the left boundary point
- Shape of Normal distribution

<!-- # Data Analysis -->

<!-- ## Dow Jones Index -->

<!-- ```{r, message=FALSE, echo=FALSE} -->
<!-- dji <- -->
<!--   read_csv("../data/dji.csv") %>% -->
<!--   select(Date, Open) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- (dji_return <- -->
<!--   dji %>% -->
<!--   mutate( -->
<!--     price_lag = dplyr::lag(Open), -->
<!--     yt = -log(Open / price_lag) -->
<!--   ) %>% -->
<!--   slice(-1)) -->
<!-- ``` -->

<!-- ## Negative log Return -->

<!-- ```{r} -->
<!-- dji_return %>% -->
<!--   ggplot(aes(x = Date, y = price_lag)) + -->
<!--   geom_line() + -->
<!--   scale_x_date() -->
<!-- ``` -->


<!-- ## CVaR -->

<!-- ```{r} -->
<!-- dji_return %>%  -->
<!--   mutate( -->
<!--     cvar = wdkll_cvar(yt ~ price_lag, data = .) %>%  -->
<!--       predict() -->
<!--   ) -->
<!-- ``` -->

# Future Study

## Bandwidth Selection

### Two bandwidths

- Initial bandwidth $h$: insensitive to the final estimation
- WNW bandwith $\lambda$

### Strategy

Use linear estimators

- WNW estimator: select one $\tilde{h}$
    - $h \le 0.1 \tilde{h}$: take small initial bandwidth
- Given $h$
    - Use $\hat{F}_{cai}$

### Criterion

- Nonparametric AIC [@cai2000application]
- GCV?

## Real Data

@cai:2008aa used Dow Jones index with daily return defined by $y_t \defn - 100 \ln \frac{P_t}{P_{t - 1}}$

```{r, message=FALSE, echo=FALSE}
dji <-
  read_csv("../data/dji.csv") %>%
  select(Date, Open)
```

```{r, include=FALSE}
dji_return <- 
  dji %>% 
  mutate(
    yt = -log(Open / dplyr::lag(Open)) * 100,
    xt = dplyr::lag(yt)
  ) %>% 
  dplyr::filter(
    Date >= "1998-11-03",
    Date <= "2006-01-03"
  )
```

```{r, echo=FALSE}
dji_return %>%
  ggplot(aes(x = Date, y = yt)) +
  geom_line() +
  scale_x_date() +
  ylab("Daily return")
```


## References




